{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648df6d9",
   "metadata": {},
   "source": [
    "## Terminal Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e4a0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./venv/lib/python3.6/site-packages (4.63.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.6/site-packages (from tqdm) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.6/site-packages (from importlib-resources->tqdm) (3.6.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp36-cp36m-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./venv/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install pyspark\n",
    "#!pip install nltk\n",
    "!pip install tqdm\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60d316",
   "metadata": {},
   "source": [
    "## Imports & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b555a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from dateutil.parser import parse\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b9321",
   "metadata": {},
   "source": [
    "## Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199ddfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_TO_IGNORE = ['bolig', 'abonnement', 'xyz 42 testseksjon', 'header_1']\n",
    "SITES_TO_IGNORE = ['kundeservice.adressa.no']\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNFREQ_TOKEN= '<UNF>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6f7d0",
   "metadata": {},
   "source": [
    "## Contextual Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54b72034",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_article_folder = \"home/lemeiz/content_refine/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bdf8b",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "523d9f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saken oppdateres. Det er Trøndelag politidistrikt som klokken 15.50 melder om at to barn er observert over tunnelåpningen Frøyatunnelen på Frøya-siden. Ifølge Twitter-meldingen er det ingen sikring på stedet og fare for at barna kan falle ned i veien. Politipatruljen fant ingen barn da de kom til stedet, kun fotspor, opplyser politiet på Twitter klokken 16.00. Nå advarer politet mot å leke i dette området. - Dette er ingen lekeplass, skriver politiet.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(path_to_article_folder, os.listdir(path_to_article_folder)[0]), 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    content_raw = json.loads(line)\n",
    "    dict_content = {}\n",
    "    for key in content_raw:\n",
    "        if key == 'fields':\n",
    "            for json_field in content_raw['fields']:\n",
    "                value = json_field['value']\n",
    "                if json_field['field'] == 'body':\n",
    "                    value = ' '.join(value)\n",
    "                dict_content[json_field['field']] = value\n",
    "        dict_content[key] = content_raw[key]\n",
    "    \n",
    "    print(dict_content['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d467e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_to_dict(line:str) -> dict:\n",
    "    content_raw = json.loads(line)\n",
    "    dict_content = {}\n",
    "    for key in content_raw:\n",
    "        if key == 'fields':\n",
    "            for json_field in content_raw['fields']:\n",
    "                value = json_field['value']\n",
    "                if json_field['field'] == 'body':\n",
    "                    value = ' '.join(value)\n",
    "                dict_content[json_field['field']] = value\n",
    "        else:\n",
    "            dict_content[key] = content_raw[key]\n",
    "    \n",
    "    return dict_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8461e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_information_parse(line:str) -> dict:\n",
    "    def unique_list_if_str(value):\n",
    "        if type(value) == list:\n",
    "            return value\n",
    "        else:\n",
    "            return [value]\n",
    "        \n",
    "        \n",
    "    #content_raw = parse_json_to_dict(line)\n",
    "    content_raw = defaultdict(str, parse_json_to_dict(line))\n",
    "    \n",
    "    publishtime = content_raw['publishtime'] if content_raw['publishtime'] != '' else content_raw['createtime'] \n",
    "    #Converting to unix timestamp in miliseconds\n",
    "    publishtime_ts = int(parse(publishtime).timestamp()) * 1000\n",
    "    \n",
    "    author_1st = content_raw['author'][0] if type(content_raw['author']) == list else content_raw['author']\n",
    "    \n",
    "    if type(content_raw['heading']) == list:\n",
    "        heading = set(content_raw['heading']) #Set to remove repeated phrases\n",
    "    else:\n",
    "        heading = [content_raw['heading']]\n",
    "    \n",
    "    textual_highlights = f\"{content_raw['title']} | {content_raw['teaser']} | {'. '.join(heading)} | {content_raw['body']}\".replace(u'\\xad','').replace('\"', '')\n",
    "    \n",
    "    new_content = {'id': content_raw['id'],\n",
    "                   'url': content_raw['url'],\n",
    "                   'site': unique_list_if_str(content_raw['og-site-name'])[0],\n",
    "                   'adressa-access': content_raw['adressa-access'], #(free, subscriber)\n",
    "                   'author_1st':  author_1st if author_1st != '' else '', #3777 unique                  \n",
    "                   'publishtime': publishtime,\n",
    "                   'created_at_ts': publishtime_ts,\n",
    "                   'text_highlights': textual_highlights, \n",
    "                   #Extracted using NLP techniques (by Adressa)\n",
    "                   'concepts': ','.join(unique_list_if_str(content_raw['kw-concept'])), #98895 unique\n",
    "                   'entities': ','.join(unique_list_if_str(content_raw['kw-entity'])), #150214 unique\n",
    "                   'locations': ','.join(unique_list_if_str(content_raw['kw-location'])), #5533 unique\n",
    "                   'persons': ','.join(unique_list_if_str(content_raw['kw-person'])), #53535 unique\n",
    "                   #Categories and keywords tagged by the journalists of Adresseavisen and may be of variable quality (label)\n",
    "                   'category0': content_raw['category0'], #39 unique\n",
    "                   'category1': content_raw['category1'] if 'category1' in content_raw else '', #126 unique\n",
    "                   'category2': content_raw['category2'] if 'category2' in content_raw else '', #75 unique\n",
    "                   'keywords': content_raw['keywords'], #6489 unique\n",
    "                  }\n",
    "\n",
    "        \n",
    "    return new_content\n",
    "\n",
    "with open(os.path.join(path_to_article_folder, os.listdir(path_to_article_folder)[0]), 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    x = article_information_parse(line)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39491e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13eb96b4cfbbc5954c54a75737afcac5ccc61779',\n",
       " 'url': 'http://www.adressa.no/nyheter/trondheim/article586450.ece',\n",
       " 'site': 'adressa.no',\n",
       " 'adressa-access': 'free',\n",
       " 'author_1st': 'elin fosshaug olsø',\n",
       " 'publishtime': '2005-10-29T14:28:40.000Z',\n",
       " 'created_at_ts': 1130596120000,\n",
       " 'text_highlights': 'Bilister aggressive mot trafikkaksjon | En varebil holdt på å kjøre ned ei barnevogn og flere fotgjengere, da beboere på Tiller aksjonerte mot trafikkaos lørdag ettermiddag. | Brå u-sving. Bilister aggressive mot trafikkaksjon -adressa.no. – Kaotisk. – Ønsker bom | Saken oppdateres. Flere bilførere reagerte med aggressiv kjøring og forsøk på å trenge seg gjennom folkemengden med bil, da omkring femti beboere på Tiller sperret trafikken på Østre Rosten med en gåsakte-aksjon på lørdag. Trafikksituasjon var høyst kaotisk midt i den verste lørdagshandelen. Også ved Rostengrenda ble trafikken sperret ved at en mengde beboere på Tiller gikk fram og tilbake over fotgjengerfeltet. Køene ble flere hundre meter lange, og flere sjåfører tok en brå u-sving eller vrengte bilene over i motsatt kjørefelt og kjørte fram til aksjonistene, som fikk en real skyllebøtte. Enkelte biler forsøkte å trenge seg gjennom folkemengden. Det kom nærmest til håndgemeng da en varebil kjørte rett på en barnevogn i et forsøk på å brøyte seg gjennom. En av aksjonslederne, Solfrid Berg Hansen, holdt på å bli slått ned av en ung mann, som etter hvert tverrsnudde. Ingen kom imidlertid til skade under aksjonen. – Det var kaotisk, men jeg føler at vi hadde kontroll over situasjonen, sier Berg Hansen. Hun delte ut løpesedler til bilistene som ble stående og stange i kø midt i lørdagstrafikken. Ifølge Berg Hansen var omtrent halvparten positivt innstilt til aksjonen, som er en reaksjon på frutsrasjon over trafikksitausjonen på Østre Rosten. – Det er nesten like stor belastning på Østre Rosten som på E6 nå, sier Berg Hansen. Beboerne på Tiller har tidligere varslet aksjoner som følge av manglende politisk forståelse for behovet for en ny E6 for å avlaste Rosten-området. – Før hørte vi ikke trafikken om kveldene. Nå er det slik at mange som kommer sørfra tar av ved Sandmoen og kjører forbi her istedenfor å kjøre E6. Og rundt Obs bygg er det rene racerbanen, sier aksjonist Trond Imset, som ønsker seg en bom som begrenser trafikken på Østre Rosten mellom kl 21 og 06. Beboerne mener situasjone forverres kraftig som et resultat av at Rema og Lidl får bygge på Rosten før infrastrukturen er tilstrekkelig utbygd. Til tross for lavpriskjedene skal bidra med enkelte tiltak for å bedre trafikksituasjonen, mener ikke leder for beboerforeningen Tiller Vel Ingebrigt Storli at det vil lette trafikktrykket. – Det vil ikke fjerne en eneste bil på Østre Rosten. Derfor er ny E6 den eneste måten å bedre situasjonen på, uttalte Storli nylig til Adresseavisen, da aksjonen ble varslet.',\n",
       " 'concepts': 'brå u-sving,bilister aggressive,trafikken,bil,aksjonen,kjøre,beboere',\n",
       " 'entities': 'obs,sandmoen,tiller vel ingebrigt storli,østre rosten,rostengrenda,adresseavisen,rema og lidl',\n",
       " 'locations': '',\n",
       " 'persons': 'berg hansen,trond imset,solfrid berg hansen',\n",
       " 'category0': 'nyheter',\n",
       " 'category1': 'nyheter|trondheim',\n",
       " 'category2': '',\n",
       " 'keywords': 'utenriks,innenriks,trondheim,E6,midtbyen,bybrann,bilulykker'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_content_file(fp:str):\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                if line.strip()=='null':\n",
    "                    return None\n",
    "                else:\n",
    "                    content=article_information_parse(line)\n",
    "                return content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "parse_content_file(os.path.join(path_to_article_folder, os.listdir(path_to_article_folder)[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dbf4e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(rel_path, file_list=None) -> pd.DataFrame:\n",
    "    if not file_list:\n",
    "        file_list = os.listdir(rel_path)\n",
    "    \n",
    "    article_data = []\n",
    "    \n",
    "    for idx, filename in enumerate(tqdm(file_list)):\n",
    "        fp = os.path.join(rel_path, filename)\n",
    "        file_content = parse_content_file(fp)\n",
    "        if file_content:\n",
    "            article_data.append(file_content)\n",
    "            \n",
    "    print(f'# Files processed       : {len(file_list)}')\n",
    "    print(f'# Files parsed          : {len(article_data)}')\n",
    "    print(f'# Files rejected (empty): {len(file_list) - len(file_content)}')\n",
    "    \n",
    "    return article_data\n",
    "    \n",
    "    \n",
    "#load_files(path_to_article_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "69f6539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_folder(rel_path):\n",
    "    articles_list = os.listdir(rel_path)\n",
    "    articles = load_files(rel_path, articles_list)\n",
    "    \n",
    "    news_df = pd.DataFrame([art for art in articles])\n",
    "    news_df = news_df[(~news_df['category0'].isin(CATEGORIES_TO_IGNORE)) & (~news_df['site'].astype(str).isin(SITES_TO_IGNORE))]\n",
    "    \n",
    "\n",
    "    news_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa7f4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_folder_path:str, output_path:str):\n",
    "    print(f'Loading data from {input_folder_path}')\n",
    "    news_df = load_from_folder(input_folder_path)\n",
    "    print(f'Loaded {len(news_df)} articles.')\n",
    "    \n",
    "    print(f'Saving data to {output_path}')\n",
    "    news_df.to_csv(output_path, index=False)\n",
    "    print(f'Saved {len(news_df)} articles.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cac6b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from home/lemeiz/content_refine/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 55910/74886 [01:12<00:24, 777.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 2 column 1 (char 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74886/74886 [01:36<00:00, 777.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Files processed       : 74886\n",
      "# Files parsed          : 74885\n",
      "# Files rejected (empty): 74870\n",
      "Loaded 73308 articles.\n",
      "Saving data to data/raw_df/adressa_raw.csv\n",
      "Saved 73308 articles.\n"
     ]
    }
   ],
   "source": [
    "preprocess(path_to_article_folder, 'data/raw_df/adressa_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "172ad380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({'a':1, 's':'d'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
